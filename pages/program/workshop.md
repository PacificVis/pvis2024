---
layout: single
title: 'Workshop'
permalink: '/program/workshop/'
date: 2023-03-20
---

**Coding**

- <span class="track track_workshop_full">full</span> Full paper (20 min)
- <span class="track track_workshop_short">short</span> Short paper (10 min)

---

# April 23rd (Tuesday)

## 14:00-15:30 Workshop Session I: AI4VIS

- 14:00-15:00 Invited talk by Chaoli Wang

    Title
    : Generative AI for Scientific Visualization

    Abstract
    : Over the past several years, generative AI for scientific visualization has quickly become a focused research direction. In this talk, I will present two of our recent works. The first work leverages multilayer perceptrons to ingest coordinates and predict quantities of interest, flexibly accomplishing diverse data generation and visualization generation tasks using the same network design and architecture. The second work utilizes a hybrid radiance field representation to efficiently train and infer rendering images given a sparse set of labeled image samples, enabling high-quality visualization synthesis across novel viewpoints, timesteps, isovalues, transfer functions, or simulation parameters. Finally, I will discuss future perspectives for this vibrant research direction.

    Biography
    : Chaoli Wang is a Professor in the Department of Computer Science and Engineering at the University of Notre Dame. He holds a Ph.D. degree in Computer and Information Science from The Ohio State University. He has published around 130 refereed papers in international journals and conferences. His recent research focuses on deep learning for scientific visualization, including representation learning, data generation, and visualization synthesis. Dr. Wang has served the professional community as a Paper Co-Chair for multiple visualization conferences (IEEE VIS, IEEE PacificVis, IEEE LDAV, ChinaVis, and ISVC), an Associate Editor of IEEE TVCG, a member of the Steering Committee of IEEE LDAV, and the International Liaison of the IEEE VGTC Executive Committee. For more information, please visit [https://sites.nd.edu/chaoli-wang/](https://sites.nd.edu/chaoli-wang/).

    <img width="50%" src="/pvis2024/assets/images/workshop/Chaoli_Wang.jpg" />

- 15:00-15:30 Full and short paper presentations

    <span class="track track_workshop_full">full</span> **Are LLMs ready for Visualization?**\
    Pere Pau VÃ¡zquez<!-- article_id: 1709834211673 -->

    <span class="track track_workshop_short">short</span> **CommentVis: Unveiling Comment Insights Through Interactive Visualization Tool**\
    Guangjing Yan, Jinhwa Jang, and Jinwook Seo<!-- article_id: 1709622345486 -->



- 15:30-16:00 Coffee break

## 16:00-17:30 Workshop Session II: VIS4AI

- 16:00-17:00 Invited talk by Siming Chen

    Title
    : Human-AI Collaborative Visual Analytics: From Explainable AI to Large Language Model Enhanced Visual Analytics

    Abstract
    : In recent years, artificial intelligence has developed rapidly. In data analysis tasks, collaboration between AI and humans has become increasingly important for problems that cannot be directly solved by AI alone. In this talk, we discuss the collaboration between humans and AI through the visual interface from two aspects: the validation of AI models, which opens the "black box" of artificial intelligence to provide "explainability", and utilizing AI to recommend interactive behaviors for visual exploration. Firstly, AI is still a "black box" for humans. Understanding the correctness of AI behavior and exploring scenarios and possible reasons for AI failure is the core of our first work. We applied our interactive AI detection method in an autonomous driving visual detection model. To further open the "black box" of AI, we explore the different parameters and layers of neural networks through visualization and attribution methods and investigate the interpretability of the model in open-domain natural language conversation. Secondly, Large Language Model (LLMs) provides the ability to interpret various forms of data, offering the potential to support intelligent visual analytics. We introduced a new framework LEVA: LLM-Enhanced Visual Analytics. It provides intelligent insight suggestions when interacting with the exploration and summarizes the story of the exploration. Through the above cases, we discuss various key points of human-AI collaboration and summarize the effectiveness that intelligent human-AI interaction needs to achieve.

    Biography
    : Dr. Siming Chen is an Associate Professor at the School of Data Science, Fudan University. He leads the Fudan Visualization Lab (FDUVIS). Before this, he was a Research Scientist at Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems) and a Postdoc Researcher at the University of Bonn in Germany. He received his Ph.D. in computer science at the School of EECS, Peking University, and received his BS degree in computer science at Fudan University. His research interests are visualization and visual analytics, with an emphasis on human-AI collaboration, social media visual analytics, and spatial-temporal visual analytics. He has published more than 100 papers, more than 30 of which are in top conferences and journals, including IEEE VIS, IEEE TVCG, ACM CHI, ACM UIST, etc. He served as multiple organizing chairs, associate editors, and program committees of several international journals and conferences. He was awarded 10+ best paper/poster awards and honorable mentioned awards in multiple conferences, including EuroVA, ChinaVis, AGILE, and IEEE VIS Poster, and won multiple IEEE VAST Challenge Excellent Awards. For more information, please visit [http://simingchen.me](http://simingchen.me).

    <img width="50%" src="/pvis2024/assets/images/workshop/Siming_Chen.jpeg" />

- 17:00-17:30 Full and short paper presentations

    <span class="track track_workshop_full">full</span> **SampleViz: Concept based Sampling for Policy Refinement in Deep Reinforcement Learning**\
    Zhaohui Liang, Guan Li, Ruiqi Gu, Yang Wang, and Guihua Shan<!-- article_id: 1709004171262 -->

    <span class="track track_workshop_short">short</span> **CLeVer: Continual Learning Visualizer for Detecting Task Transition Failure**\
    Minsuk Chang, Donghun Kim, Hyeon Jeon, Seokweon Jung, and Jinwook Seo<!-- article_id: 1709009457348 -->

